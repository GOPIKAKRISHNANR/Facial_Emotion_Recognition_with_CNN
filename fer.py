# -*- coding: utf-8 -*-
"""FER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mepdHN7sc18Ncdae-pN3Dq3BbsmrCM_r
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from keras.layers import InputLayer
import random
from tqdm.notebook import tqdm
warnings.filterwarnings('ignore')
import tensorflow as tf
from PIL import Image
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D

TRAIN_DIR = '/content/drive/MyDrive/Mini_Project_Facial_Exp/train'
TEST_DIR = '/content/drive/MyDrive/Mini_Project_Facial_Exp/test'
model_dir = 'Trained Models/model.h5'

def load_dataset(directory):
    image_paths = []
    labels = []
    for label in os.listdir(directory):
        for filename in os.listdir(os.path.join(directory, label)):
            image_path = os.path.join(directory, label, filename)
            image_paths.append(image_path)
            labels.append(label)
    return image_paths, labels

train = pd.DataFrame()
train['image'], train['label'] = load_dataset(TRAIN_DIR)
train = train.sample(frac=1).reset_index(drop=True)

test = pd.DataFrame()
test['image'], test['label'] = load_dataset(TEST_DIR)

label_encoder = LabelEncoder()
train['label_encoded'] = label_encoder.fit_transform(train['label'])

# Plot class distribution
sns.countplot(data=train, x='label_encoded')
tick_labels = label_encoder.inverse_transform(range(len(label_encoder.classes_)))
plt.xticks(range(len(label_encoder.classes_)), tick_labels, rotation=45)
plt.show()

# Visualize a sample image
img = Image.open(train['image'][0])
plt.imshow(img, cmap='gray')
plt.show()

files = train.iloc[0:25]

for index, row in files.iterrows():
    plt.subplot(5, 5, index + 1)
    img = load_img(row['image'])
    plt.imshow(img)
    plt.title(row['label'])
    plt.axis('off')

plt.tight_layout()
plt.show()

"""Feature extraction"""

def extract_features(images):
    features = []
    for image in tqdm(images):
        img = load_img(image, color_mode='grayscale', target_size=(48, 48))
        img = np.array(img)
        features.append(img)
    features = np.array(features)
    features = features.reshape(len(features), 48, 48, 1)
    return features

train_features = extract_features(train['image'].iloc[:5000])
with open('train_features.pkl', 'wb') as f:
    pickle.dump(train_features, f)

print("Train features saved successfully.")

test_features = extract_features(test['image'].iloc[:5000])
with open('test_features.pkl', 'wb') as f:
    pickle.dump(test_features, f)

print("Test features saved successfully.")

# In the cell where you define and prepare your training and testing data (likely ipython-input-9-7ca330a34abe)
x_train = train_features / 255.0
x_test = test_features / 255.0

# Ensure y_train and y_test are created using the same number of samples as x_train and x_test
y_train_labels = train['label'].iloc[:5000]  # Take only the first 5000 labels
y_test_labels = test['label'].iloc[:5000]  # Take only the first 5000 labels (adjust if necessary based on test_features size)

y_train_encoded = label_encoder.transform(y_train_labels)
y_test_encoded = label_encoder.transform(y_test_labels)

output_class = len(label_encoder.classes_)

y_train = to_categorical(y_train_encoded, num_classes=output_class)
y_test = to_categorical(y_test_encoded, num_classes=output_class)

print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
print("x_test shape:", x_test.shape)
print("y_test shape:", y_test.shape)

input_shape = (48, 48, 1)

model = Sequential()
model.add(InputLayer(input_shape=input_shape))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.4))

model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.4))

model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.4))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.4))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(output_class, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print the model architecture
print("\nModel Architecture:")
model.summary()

# Train the model
history = model.fit(
    x_train, y_train,
    batch_size=512,
    epochs=75,  # Adjust as needed
    validation_data=(x_test, y_test),
    verbose=1
)

# Display epoch-wise metrics
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

print("\nTraining Summary:")
for epoch in range(len(acc)):
    print(f"Epoch {epoch + 1}:")
    print(f"  Training Accuracy: {acc[epoch]:.4f}")
    print(f"  Validation Accuracy: {val_acc[epoch]:.4f}")
    print(f"  Training Loss: {loss[epoch]:.4f}")
    print(f"  Validation Loss: {val_loss[epoch]:.4f}")
    print()

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)

# Print final results
print("\nFinal Results:")
print(f"  Training Accuracy: {acc[-1]:.4f}")
print(f"  Validation Accuracy: {val_acc[-1]:.4f}")
print(f"  Test Accuracy: {test_accuracy:.4f}")
print(f"  Training Loss: {loss[-1]:.4f}")
print(f"  Validation Loss: {val_loss[-1]:.4f}")

# Plot metrics
epochs = range(len(acc))
plt.figure(figsize=(14, 5))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label='Training Accuracy', color='blue')
plt.plot(epochs, val_acc, label='Validation Accuracy', color='red')
plt.title('Accuracy')
plt.legend()

# Loss Plot
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label='Training Loss', color='blue')
plt.plot(epochs, val_loss, label='Validation Loss', color='red')
plt.title('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Save the trained model
os.makedirs('/content/drive/MyDrive/Trained_Models', exist_ok=True)
model.save(model_dir)
print(f"Model saved at: {model_dir}")

!pip install gradio --quiet

import gradio as gr
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np

# Load the trained model
model = tf.keras.models.load_model(model_dir)

# Define the prediction function
def predict_expression(image):
    img = load_img(image, target_size=(48, 48), color_mode='grayscale')
    img_array = img_to_array(img)
    img_array = img_array.reshape(1, 48, 48, 1) / 255.0
    prediction = model.predict(img_array)
    predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])
    return predicted_label[0]

# Create and launch the Gradio interface
interface = gr.Interface(
    fn=predict_expression,
    inputs=gr.Image(type="filepath", label="Upload an Image"),
    outputs=gr.Text(label="Predicted Expression"),
    title="Facial Expression Recognition",
    description="Upload a grayscale image (48x48 pixels) to predict the facial expression."
)

interface.launch()